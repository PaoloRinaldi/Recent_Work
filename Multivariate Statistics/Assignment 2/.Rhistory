task1
load("C:/Users/Paolo/Desktop/R KUL Courses/Multivariate Statistics/Assignment 2 Multiv Stat/task1.Rdata")
View(train.data.s1)
View(train.data.s1)
View(train.data.s1)
test.target.s1
train.target.s1
train.data.s1
library(rstudioapi)
library(paran)
ztrains1 <- scale(train.data.s1, center=TRUE, scale=TRUE)
ztrains2 <- scale(train.data.s2, center=TRUE, scale=TRUE)
ztrains1
head(train.data.s1)
head(ztrains1)
head(train.data.s1, 10)
head(ztrains1, 10)
pca_s1 <- prcomp(train.data.s1)
pca_s1
prop_var_s1 <- summary(pca_s1)$importance[3, ]
num_comp_s1 <- min(which(prop_var_s1 >= 0.90))
num_comp_s1
pca_s2 <- prcomp(train.data.s2)
prop_var_s2 <- summary(pca_s2)$importance[3, ]
num_comp_s2 <- min(which(prop_var_s2 >= 0.90))
num_comp_s2
train_pca_s1 <- pca_s1$x[, 1:num_comp_s1]
test_pca_s1 <- predict(pca_s1, newdata = test.data)[, 1:num_comp_s1]
train_pca_s1
test_pca_s1
train_pca_s2 <- pca_s2$x[, 1:num_comp_s2]
test_pca_s2 <- predict(pca_s2, newdata = test.data)[, 1:num_comp_s2]
train_pca_s2
test_pca_s2
fit.lda.s1 <- lda(y ~ ., data = train_df_s1)
fit.lda.s1 <- lda(y ~ ., data = train_df_s1)
fit.lda.s1 <- lda(y ~ ., data = train_pca_s1)
train_df_s1 <- data.frame(train_pca_s1, y = train.target.s1)
test_df_s1  <- data.frame(test_pca_s1,  y = test.target)
train_df_s1 <- data.frame(train_pca_s1, y = train.target.s1)
test_df_s1  <- data.frame(test_pca_s1,  y = test.target)
fit.lda.s1 <- lda(y ~ ., data = train_df_s1)
pred.lda.s1 <- predict(fit.lda.s1, train_df_s1)
table_train_s1 <- table(observed = train_df_s1$y, predicted = pred.lda.s1$class)
print(table_train_s1)
err_train_lda_s1 <- mean(pred.lda.s1$class != train_df_s1$y)
err_train_lda_s1
1-sum(diag(table_train_s1))/sum(table_train_s1)
1-sum(diag(table_train_s1))/sum(table_train_s1)
qda.out.s1<-qda(y ~ ., data = train_df_s1)
qda.out.s1<-qda(y ~ ., data = train_df_s1)
pred.qda.s1 <- predict(qda.out.s1, train_df_s1)
table_train__qda_s1 <- table(observed = train_df_s1$y, predicted = pred.qda.s1$class)
err_train_qda_s1 <- mean(pred.qda.s1$class != train_df_s1$y)
err_train_qda_s1
1-sum(diag(table_train_qda_s1))/sum(table_train_s1)
1-sum(diag(table_train_qda_s1))/sum(table_train_qda_s1)
1-sum(diag(table_train__qda_s1))/sum(table_train__qda_s1)
qda.out.s1<-qda(y ~ ., data = train_df_s1)
qda.out.s1<-qda(y ~ ., data = train_df_s1)
pred.qda.s1 <- predict(qda.out.s1, train_df_s1)
table_train_qda_s1 <- table(observed = train_df_s1$y, predicted = pred.qda.s1$class)
err_train_qda_s1 <- mean(pred.qda.s1$class != train_df_s1$y)
err_train_qda_s1
1-sum(diag(table_train_qda_s1))/sum(table_train_qda_s1)
train_df_s1 <- data.frame(train_pca_s1, y = train.target.s1)
test_df_s1  <- data.frame(test_pca_s1,  y = test.target)
# FIT LDA MODEL on scenario 1 and provide confusion matrix and classification error
lda.out.s1 <- lda(y ~ ., data = train_df_s1)
pred.lda.s1 <- predict(lda.out.s1, train_df_s1)
table_train_s1 <- table(observed = train_df_s1$y, predicted = pred.lda.s1$class)
print(table_train_s1)
# err Gemini's method
err_train_lda_s1 <- mean(pred.lda.s1$class != train_df_s1$y)
err_train_lda_s1
# err Professor's method
1-sum(diag(table_train_s1))/sum(table_train_s1)
# FIT QDA MODEL on scenario 1 and provide confusion matrix and classification error
qda.out.s1<-qda(y ~ ., data = train_df_s1)
pred.qda.s1 <- predict(qda.out.s1, train_df_s1)
table_train_qda_s1 <- table(observed = train_df_s1$y, predicted = pred.qda.s1$class)
print(table_train_qda_s1)
# err Gemini's method
err_train_qda_s1 <- mean(pred.qda.s1$class != train_df_s1$y)
err_train_qda_s1
# err Professor's method
1-sum(diag(table_train_qda_s1))/sum(table_train_qda_s1)
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- c(1, 3, 5, 7, 9, 11, 15, 21)
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
library(MASS)
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- c(1, 3, 5, 7, 9, 11, 15, 21)
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
library(gbm)
install.packages(gbm)
install.packages("gbm")
library(gbm)
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- c(1, 3, 5, 7, 9, 11, 15, 21)
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
library(class)
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- c(1, 3, 5, 7, 9, 11, 15, 21)
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
# Identifichiamo il miglior K
best_k_s1 <- k_values[which.min(test_errors)]
cat("Miglior K per Scenario 1:", best_k_s1, "\n")
# 1. Creiamo un data frame con i risultati per analizzarli meglio
results_knn <- data.frame(k = k_values, test_error = test_errors)
# 2. Ordiniamo i risultati per errore crescente
# Il primo della lista sarà il 'best_k', il secondo sarà il 'second best'
sorted_results <- results_knn[order(results_knn$test_error), ]
cat("Classifica dei migliori K per lo Scenario 1:\n")
print(head(sorted_results, 3)) # Mostra i primi 3
# 3. Visualizzazione grafica (fondamentale per la discussione dei risultati)
plot(k_values, test_errors, type = "b", pch = 19, col = "blue",
xlab = "Valore di K", ylab = "Errore di Test",
main = "Tuning di KNN: Errore di Test vs K")
grid()
[cite_start]library(class) # [cite: 31]
[cite_start]library(class) # [cite: 31]
# 1. Definiamo i valori di K da testare (da 1 a 50 per una curva più fluida)
k_values_graph <- c(1, 3, 5, 7, 9, 11, 15, 21, 31, 41, 51)
train_err_vec <- numeric(length(k_values_graph))
test_err_vec  <- numeric(length(k_values_graph))
# 2. Ciclo per calcolare entrambi gli errori
for(i in 1:length(k_values_graph)) {
k_curr <- k_values_graph[i]
# Errore di Training
pred_train <- knn(train = X_train_s1, test = X_train_s1, cl = Y_train_s1, k = k_curr)
train_err_vec[i] <- mean(pred_train != Y_train_s1)
# Errore di Test
pred_test <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_curr)
test_err_vec[i] <- mean(pred_test != test.target)
}
# 3. Trasformazione asse X: 1/K
inv_k <- 1 / k_values_graph #
# 4. Creazione del grafico (Metodologia Slide 25)
# Iniziamo con la curva dell'errore di Training (Blu)
plot(inv_k, train_err_vec, type = "b", col = "blue", pch = 19,
ylim = c(0, max(test_err_vec)), # Imposta i limiti verticali
xlab = "1/K (Flessibilità)", ylab = "Errore di Classificazione",
main = "Errore di Training vs Test (KNN)")
# Aggiungiamo la curva dell'errore di Test (Rosso)
lines(inv_k, test_err_vec, type = "b", col = "red", pch = 19)
# Aggiungiamo la legenda
legend("topleft", legend = c("Training Error", "Test Error"),
col = c("blue", "red"), lty = 1, pch = 19)
grid()
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- 300
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
# Identifichiamo il miglior K
best_k_s1 <- k_values[which.min(test_errors)]
cat("Miglior K per Scenario 1:", best_k_s1, "\n")
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- 300
test_errors <- numeric(length(k_values))
for(i in 1:length(k_values)) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
# Identifichiamo il miglior K
best_k_s1 <- k_values[which.min(test_errors)]
cat("Miglior K per Scenario 1:", best_k_s1, "\n")
# 1. Definiamo i valori di K da testare (da 1 a 50 per una curva più fluida)
k_values_graph <- c(1, 3, 5, 7, 9, 11, 15, 21, 31, 41, 51)
train_err_vec <- numeric(length(k_values_graph))
test_err_vec  <- numeric(length(k_values_graph))
# 2. Ciclo per calcolare entrambi gli errori
for(i in 1:length(k_values_graph)) {
k_curr <- k_values_graph[i]
# Errore di Training
pred_train <- knn(train = X_train_s1, test = X_train_s1, cl = Y_train_s1, k = k_curr)
train_err_vec[i] <- mean(pred_train != Y_train_s1)
# Errore di Test
pred_test <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_curr)
test_err_vec[i] <- mean(pred_test != test.target)
}
# 3. Trasformazione asse X: 1/K
inv_k <- 1 / k_values_graph #
# 4. Creazione del grafico (Metodologia Slide 25)
# Iniziamo con la curva dell'errore di Training (Blu)
plot(inv_k, train_err_vec, type = "b", col = "blue", pch = 19,
ylim = c(0, max(test_err_vec)), # Imposta i limiti verticali
xlab = "1/K (Flessibilità)", ylab = "Errore di Classificazione",
main = "Errore di Training vs Test (KNN)")
# Aggiungiamo la curva dell'errore di Test (Rosso)
lines(inv_k, test_err_vec, type = "b", col = "red", pch = 19)
# Aggiungiamo la legenda
legend("topleft", legend = c("Training Error", "Test Error"),
col = c("blue", "red"), lty = 1, pch = 19)
grid()
X_train_s1 <- train_pca_s1
X_test_s1  <- test_pca_s1
Y_train_s1 <- train.target.s1
k_values <- 300
test_errors <- numeric(length(k_values))
for(i in 1:k_values) {
pred_tmp <- knn(train = X_train_s1, test = X_test_s1, cl = Y_train_s1, k = k_values[i])
test_errors[i] <- mean(pred_tmp != test.target)
}
# Definiamo il numero massimo di K da testare
knnmax <- 100
# Inizializziamo la matrice degli errori (Riga = K, Colonna 1 = Training, Colonna 2 = Test)
err_s1 <- matrix(rep(0, knnmax * 2), nrow = knnmax)
# Ciclo KNN seguendo il tuo modello
for (j in 1:knnmax) {
# Predizione sul Training Set per l'errore di training
predknn.train <- knn(train = train_pca_s1, test = train_pca_s1, cl = train.target.s1, k = j)
err_s1[j, 1] <- mean(predknn.train != train.target.s1) # Corrisponde a 1 - hitrate
# Predizione sul Test Set per l'errore di test
predknn.test <- knn(train = train_pca_s1, test = test_pca_s1, cl = train.target.s1, k = j)
err_s1[j, 2] <- mean(predknn.test != test.target) # Corrisponde a 1 - hitrate
}
# Identifichiamo il miglior K (quello che minimizza l'errore di test, colonna 2)
best_k_s1 <- which.min(err_s1[, 2])
cat("Miglior K per lo Scenario 1:", best_k_s1, "con errore:", err_s1[best_k_s1, 2], "\n")
